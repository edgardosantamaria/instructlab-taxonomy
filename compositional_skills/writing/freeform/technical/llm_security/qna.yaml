created_by: sidhpurwala-huzaifa
seed_examples:
- answer: '# Security risks associated with using an Large Language Model (LLM)
      based application


      ## Executive Summary

      The amount of interest in Large Language Models (LLMs) following the release of popular pre-trained chatbots has been remarkable. Businesses are eager to harness the potential of LLMs in applications as diverse as operations and client-facing bots. However like all emerging technologies, the adoption has outpaced the establishment of security protocols deep security research, leaving a large number of these applications vulnerable to security risks.


      ## 1. Prompt Injection

      Prompt injection flaw occurs when the attacker is able to manipulate the LLM via crafted inputs, causing it to execute what the attacker intended. There are two ways to achieve this, either via jail-breaking the system prompt or by manipulating external inputs, leading to data exfiltration and other issues.


      ## 2. Insecure Output Handling

      Insecure Output Handling refers specifically to insufficient validation, sanitization, and handling of the outputs generated by large language models before they are passed downstream to other components and systems.


      ## 3. Data poisoning

      Data poisoning refers to manipulation of pre-training data or data involved within the fine-tuning processes to introduce vulnerabilities, backdoors or biases that could compromise the modelâ€™s security, effectiveness or ethical behavior. Poisoned information may be surfaced to users or create other risks like performance degradation, downstream software exploitation and reputational damage.

      ## 4. Model Theft

      This entry refers to the unauthorized access and exfiltration of LLM models by malicious actors. This arises when the proprietary LLM models (being valuable intellectual property), are compromised, physically stolen, copied or weights and parameters are extracted to create a functional equivalent.

      ## 5. Overreliance

      Overreliance can occur when an LLM produces erroneous information and provides it in an authoritative manner. While LLMs can produce creative and informative content, they can also generate content that is factually incorrect, inappropriate or unsafe. This is referred to as hallucination or confabulation.

      ## 6. Excessive Agency

      In this security risk, the LLM application is granted excessive degree of agency by its developer. Agency implies abilty to interface with other systems and take actions in response to a prompt.

      ## 7. Insecure Plugin Design

      LLM plugins are extensions that, when enabled, are called automatically by the model during user interactions. The model integration platform drives them, and the application may have no control over the execution, especially when the model is hosted by another party. Furthermore, plugins are likely to implement free-text inputs from the model with no validation or type-checking to deal with context-size limitations. This allows a potential attacker to construct a malicious request to the plugin, which could result in a wide range of undesired behaviors, up to and including remote code execution.

      ## 8. Sensitive Information Disclosure

      LLM applications have the potential to reveal sensitive information, proprietary algorithms, or other confidential details through their output. This can result in unauthorized access to sensitive data, intellectual property, privacy violations, and other security breaches. It is important for consumers of LLM applications to be aware of how to safely interact with LLMs and identify the risks associated with unintentionally inputting sensitive data that may be subsequently returned by the LLM in output elsewhere.


      ## 9. Supply Chain Vulnerabilities

      The supply chain in LLMs can be vulnerable, impacting the integrity of training data, ML models, and deployment platforms. These vulnerabilities can lead to biased outcomes, security breaches, or even complete system failures. Traditionally, vulnerabilities are focused on software components, but Machine Learning extends this with the pre-trained models and training data supplied by third parties susceptible to tampering and poisoning attacks.

      ## 10. Model Denial of Service

      An attacker interacts with an LLM in a method that consumes an exceptionally high amount of resources, which results in a decline in the quality of service for them and other users, as well as potentially incurring high resource costs.'

  question: What are the security risks associated with using an LLM based application.

- answer: ' Enforce privilege control on LLM access to backend systems. Provide the LLM with its
own API tokens for extensible functionality, such as plugins, data access, and function-
level permissions. Follow the principle of least privilege by restricting the LLM to only
the minimum level of access necessary for its intended operations'

  question: What is the mitigation for prompt injection in LLM applications.

- answer: ' Verify the supply chain of the training data, especially when sourced externally as well
as maintaining attestations via the "ML-BOM" (Machine Learning Bill of Materials)
methodology as well as verifying model cards'

  question: Whatis the mitigation for data poisoning in LLM applications.

- answer: ' Implement strong access controls (E.G., RBAC and rule of least privilege) and strong
authentication mechanisms to limit unauthorized access to LLM model repositories
and training environments'

  question: What is the mitigation for model theft in LLM applications.

task_description: ''

