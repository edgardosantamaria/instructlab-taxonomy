created_by: jmelis
seed_examples:
- answer: |
    The incident happened on July 31st, 2023 at 20:16 UTC, when the customer reported that their workloads were experiencing severe and critical impacts. Despite efforts to formulate a plan with SRE for correct application of podPidsLimit during a cluster upgrade, the PIDs limit was not applied as expected, leading to degraded performance of customer workloads.

    The impact of the incident was severe and critical, as it impacted customer workloads.

    The root cause of the issue was traced to the configuration setup within the machineconfigpool, which included machineconfigs from both "compute" and "worker" roles. Due to alphabetical ordering during configuration rendering, the settings from "worker" machineconfigs were prioritized over those from "compute," resulting in the incorrect PIDs limit being applied to the nodes.

    To address the root cause, immediate corrective actions were taken. Direct instructions were provided to ensure the correct PIDs limit was applied to the customer's clusters. Additionally, a ticket was opened to update documentation regarding configuration overrides and rendering order to prevent similar incidents in the future. Secondary actions aimed at improving resilience included discussions about building tools for easier identification of rendered configurations and exploring the possibility of alerts for potential conflicts in machineconfig rendering.

  context: |
    RCA: 2023-07-31  Incorrect PIDs Limit

    Executive Summary
    As per an existing support exception SRE and the customer worked together to formulate a plan around the correct application of a PodsPidLimit on their clusters. In preparation for an upgrade SRE and the customer worked together to move from ContainerRuntimeConfig to KubeletConfig. After beginning a cluster upgrade the customer reported their 4.11 clusters had the incorrect PIDs limit set on their nodes and reached out to SRE for help for further assistance.

    SRE confirmed that the nodes had the incorrect PIDs limit set in the generated Machine Config for the compute and data-model-converter Machine Config Pool.
    Incident Details

    Incident Details
    Problem Investigation Manager(s)
    Jane Smith
    Team(s)
    SRE
    Incident start date/time
    2023-07-31 20:16 UTC
    Incident end date/time
    2023-08-01 15:17 UTC
    Support Cases / Tickets/
    Security rel.  Incidents*
    SUPPORT-0001 / SUPPORT-0002
    Incident Severity
    Sev3
    Incident Owner
    Sarah Davis
    RCA Author
    Sarah Davis & Jane Smith

    Customer Impact
    Customer reported their workloads were severely and critically impacted due to the lower PIDs limit being applied to their compute nodes.
    Service Impact
    Service
    Impact (Operational, Degraded, Unavailable)
    API Endpoint / Console
    Operational
    Router / Ingress
    Operational
    On-cluster Registry
    Operational
    Platform Monitoring (SRE Prometheus instance)
    Operational
    Customer workloads
    Reported Degraded

    Root Cause
    Summary
    The customer requires a higher than standard podPidsLimit for their clusters which is achieved using a CustomContainerRuntime configuration in OpenShift v4.10 and a KubeletConfig configuration in OpenShift v4.11. The customer had both of these in place and during an upgrade of their clusters determined that the KubeletConfig podPidsLimit change was not being applied to their specified nodes within the designated machine config pools. SRE determined that this was due to the way in which rendered machine configurations are generated from the sum total of MachineConfigs, specifically that the custom podPidsLimit in a generated config was loaded before the default podPidsLimit for default workers was then applied. The customer's machine config pools were named 'compute', which is a lower lexical value than the default 'worker' pool. SRE requested the customer delete their custom machine configuration pools and create them again with names of a higher lexical value such as 'z-compute'.
    Five Whys

    1st. Why
    Why were the PIDs Limit not applied to the expected nodes? The generated config set the config at 4096 pids.
    2nd Why
    Why were the PIDs Limit set to 4096? Because the default kubeletconfig had a generated value of 4096 which overrode values generated by any other configurations
    3rd Why
    Why was the default limits configuration overwriting the high-pid-limits configuration? Because the machineconfigpool was correctly set up to include machineconfigs with both 'worker' AND 'compute' roles, and the resulting config was produced in alphabetical order.
    4th Why
    Why does the alphabetical order matter? The 'compute' kubeletconfig that was generated with the correct value was loaded before the 'worker' config with the incorrect value.

    Root cause
    The machineconfigpool was set up to include machineconfigs from both the 'compute' and 'worker' roles which due to alphabetical ordering resulted in the configuration from the 'worker' machineconfigs overriding configuration from the 'compute' machine configs.

    Root Cause Corrective Actions
    These actions directly remediated the root cause and ended the incident.

    Priority
    Issue
    Corrective Action
    Owner
    Result
    Completion Date
    Notes
    Highest
    PIDs limit not applied on customer's clusters
    Provide direct instructions to ensure the pid limit is applied
    Sarah Davis
    IcM updated with resolution information
    02-Aug-2023

    High
    Machine Config Pool documentation does not mention the possibility of configuration overrides or the order in which rendered configurations are built.
    Open ticket to update the documentation to mention these things.
    SRE
    Jira Ticket Created
    02-Aug-2023
    https://jira.example.com/browse/DOCS-0001

    Secondary Corrective Actions

    Actions in this section avoid reoccurrence of a similar incident by improving resilience or more permanently fixing the root cause. When reoccurrence cannot be entirely eliminated, these actions aim to reduce the MTTR of such an occurrence with improved monitoring and tools.

    Priority
    Issue
    Corrective Action
    Owner
    Result
    Completion Date
    Notes
    Low
    Rendered config is difficult to identify if desired config has been applied
    (Internal Only) Build a tool to quickly extract rendered config and make it available to customers
    SRE
    Discuss the necessity of this with the team.
    02-Aug-2023
    Rendered config can be checked for the PIDs Limit using the command listed in
    Low
    There are no alerts when when machineconfig is overwritten by another in the final rendered config
    (Internal Only) Perhaps we should have the machineconfigpool list in a rendered config potential conflicts for quick identification
    Machine Config Operator Team
    Discuss the necessity of this with the SRE team.
    02-Aug-2023

    Medium
    KCS made no mention of potential issues when creating custom kubeletconfig
    Review KCS and ensure that there are no missing points of information
    Emily Brown
    Jira Ticket Created
    KCS Update in Progress
    In Progress

    Low
    Initial advice provided to customer contained factually incorrect information.
    None
    Accounts Team
    https://jira.example.com/browse/EXCEPTION-0001
    N/A
    Work around the support exception is the responsibility of both Customer1 and SRE to validate functionality.

    Investigation Details/Activities

    Service or Application
    Impact Description
    Triggering Event
    Notes
    Platform
    Customer reported workloads affected
    Customer Case

  question: Generate a summary for the given RCA.

- answer: |
    The incident happened on Jan 31, 2024 at 18:43:03 PM UTC, when the customer reported that the cluster console was down and developers could not use the system.

    The impact of the incident was severe, with the cluster console being inaccessible. Developers who were attempting to triage issues with their workloads could not gain access.

    The root cause analysis revealed that the router pods lost connectivity to other pods in the hosted control plane because HAProxy routed requests from the external console hostname to a 'null' backend (NOSRV). The console was inaccessible because the connection between the router pods and backing pods was lost. The router pods were degraded because as per BUG-0001, the issue appears to be with how the router pods handle DNS pod restarts. The DNS pod restarts are not handled well because HAProxy will route requests to backends based on server name. This server name is the address of the last connection sent to the backend server. If this request is terminated mid-connection, the server name will be NOSRV. This means that any request to that DNS name will fail. The request terminated mid-connection because the DNS router pods either did not shut down gracefully (and wait for open connections to close before sending an exit signal) or the pods were force-terminated while this was happening. This likely happened because of resource starvation on the node. In this particular case, it is possible that the SRE on-call used --grace-period zero.

    To address the incident and prevent similar occurrences in the future, several corrective actions were proposed. Firstly, the immediate action of terminating and replacing the router pods resolved the connectivity issue. Additionally, secondary corrective actions were outlined to improve resilience and prevent reoccurrence, such as updating SOP documentation to include BUG-0001, creating context-gathering scripts for high-impact alerts, backporting the fix to version 4.14, and investigating why certain alerts, like ClusterMetricsMissing, did not fire as expected during the incident.

  context: |
    ITN-0001
    Cluster Console Inaccessible
    ISSUE-0001
    Executive Summary
    On Jan 31, 2024 at 18:43:03 PM UTC the customer reported the cluster console was down and developers could not use the system. SRE began their investigation and identified the root cause to be BUG-0001. This bug causes router pods in the hosted control plane to lose connectivity to API and console pods on version 4.14 clusters. This is fixed in 4.15 release candidates.  The router pods were terminated and replaced, which resolved the incident.
    ________________
    Incident Details
    Incident Details
    Problem Investigation Manager(s)
    Jane Smith
    Team(s)
    SRE
    Incident start date/time
    2024-01-31 19:35 UTC
    Incident end date/time
    2024-02-01 00:43 UTC
    Support Cases / INCIDENTs/
    Security rel.  Incidents*
    ISSUE-0001 / 0001
    Incident Severity
    1
    Incident Owner
    Mary Johnson
    RCA Author
    Robert Brown, Emily Davis, Michael Thompson
    Customer Impact
    The cluster console was inaccessible. Developers who were attempting to triage issues with their workloads could not gain access. The root cause of the incident appears unrelated to the issues with the customer workloads.
    Service Impact
    Service
    Impact (Operational, Degraded, Unavailable)
    API Endpoint / Console
    Unavailable
    Router / Ingress
    Degraded
    On-cluster Registry
    Unknown
    Platform Monitoring (Prometheus instance)
    Unknown
    Customer workloads
    Unknown

    Root Cause
    Summary
    This cluster was affected by BUG-0001, which is fixed in 4.15 release candidates. In clusters affected by this bug, the router pods in the hosted control plane lose connectivity to console and API server pods. To remediate, the router pods must be terminated and replaced.
    Five Whys
    Problem Statement 1: The console was inaccessible.
    1st. Why
    Why was the console inaccessible?  The connection between the router pods and backing pods was lost.
    2nd Why
    Why were the router pods degraded? Per BUG-0001, the issue appears to be with how the router pods handle DNS pod restarts.
    3rd Why
    Why don't the router pods handle DNS pod restarts well?
    Per BUG-0001, HAProxy will route requests to backends based on server name. This server name is the address of the last connection sent to the backend server. If this request is terminated mid-connection, the server name will be NOSRV. This means that any request to that DNS name will fail.
    4th Why
    Why was the request terminated mid-connection?
    Because the DNS router pods either do not shut down gracefully (and wait for open connections to close before sending an exit signal) or the pod was force-terminated while this was happening.
    5th Why
    Why was the pod force-terminated?
    In the original bug, this likely happened because of resource starvation on the node. In this particular case, it is possible that the SRE on-call used --grace-period zero.
    Root cause
    The router pods lost connectivity to other pods in the hosted control plane because HAProxy routed requests from the external console hostname to a 'null' backend (NOSRV)


    Problem Statement 2: The simple remediation was not known quickly.
    1st. Why
    Our SOP/indicator for this bug is documented here: https://git.example.com/sre/sop/blob/master/alert.md#BUG-0001 which is for api-ErrorBudgetBurn. api-ErrorBudgetBurn was not firing and the customer self-reported console availability issues (even though the Kube API Server would also have been degraded).
    2nd Why
    Why was api-ErrorBudgetBurn not firing/Why did the customer have to self-report console availability issues?

    The cluster had no worker nodes and so the blackbox probes by route-monitor-operator were not running. The backstop for this situation is supposed to be ClusterMetricsMissing.
    Similar to console - some customer applications probably had PDBs. The cluster was still sending metrics (even though worker nodes were stuck deleting).
    Blackbox exporter is only a single pod (which makes sense as to why the errorBudgetBurn alerts were not firing).
    3rd Why
    Why was ClusterMetricsMissing not firing?


    Probably because there were no worker nodes.
    4th Why
    5th Why
    Root cause
    ________________
    Root Cause Corrective Actions
    These actions directly remediated the root cause and ended the incident.
    Add additional lines as necessary

    Priority
    Issue
    Corrective Action
    Owner
    Result
    Completion Date
    Notes
    High
    Router pods lost connectivity to other pods in hosted control plane
    Terminated (oc delete pod) the router pods.
    SRE
    Connectivity was restored and the incident was resolved.
    2024-02-01 UTC

    Secondary Corrective Actions
    Actions in this section avoid reoccurrence of a similar incident by improving resilience or more permanently fixing the root cause. When reoccurrence cannot be entirely eliminated, these actions aim to reduce the MTTR of such an occurrence with improved monitoring and tools.
    Add additional lines as necessary

    Priority
    Issue
    Corrective Action
    Owner
    Result
    Completion Date
    Notes
    High
    console-ErrorBudgetBurn SOP does not mention BUG-0001
    Copy and/or link from the referencing section in api-ErrorBudgetBurn SOP
    SRE

    Medium

    Create context-gathering script for {api,console}-ErrorBudgetBurn.
    SRE

    This script would be executable by the on-call SREs to gather context such as pod state and logs for this high impact alerts. Depends on https://jira.example.com/browse/INCIDENT-00001 and https://jira.example.com/browse/INCIDENT-00002.
    Medium
    This is a 4.14 cluster, but the fix is in 4.15.
    Backport the fix to 4.14
    Hypershift

    https://jira.example.com/browse/BUG-0001?focusedId=00000001&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-1

    clusterMetricsMissing did not fire in this situation when it was supposed to when the workers were missing
    Investigate why clustermetricsMissing alert did not fire in this situation and fix it. When HCP worker nodes are not up this is our only current signal to tell us about something being wrong
    SRE

    Look at force-upgrading affected clusters to 4.14.5+

    https://jira.example.com/browse/INCIDENT-30140

    Investigation Details/Activities

    Service or Application
    Impact Description
    Triggering Event
    Notes
    Event Timeline (UTC)

  question: Generate a summary for the given RCA.

task_description: |
  This skill provides the ability to summarize an RCA into when it happened, what was the impact, what was the root cause and what are the corrective actions.


